# api/modules/search/routes/search.py
# Search endpoint with Smart and Full Re-Ranking modes

import logging
import time
from fastapi import APIRouter, Depends, HTTPException
from typing import Dict, List

from api.modules.search.models.schemas import SearchRequest, SearchResponse, ErrorResponse, RerankMode
from api.core.dependencies import get_system_components, SystemComponents

logger = logging.getLogger(__name__)

router = APIRouter()

# Minimum relevance threshold for considering documents as valid results
MIN_RELEVANCE_THRESHOLD = 0.6


async def generate_system_info_answer() -> str:
    """Generate answer for direct system questions"""
    return """I am a Production RAG (Retrieval-Augmented Generation) System powered by Gemini API.

My capabilities include:
- Hybrid Search: Combining database exact matching with vector semantic search
- Entity Extraction: Using AI to understand what you're looking for
- Intelligent Query Rewriting: Generating multiple search variants
- Multi-Strategy Retrieval: Searching across different data sources
- Smart AI Re-Ranking: Automatically skips LLM when not needed (saves time & tokens!)
- Advanced Results Fusion: Ranking and combining results intelligently

I specialize in searching through your document knowledge base. Ask me questions about the content in your documents, and I'll find the most relevant information.

For example:
- "Tell me about [person name]"
- "What information do we have about [topic]?"
- "Find documents related to [subject]"

How can I help you search your documents today?"""


async def generate_ai_fallback_answer(query: str, system_components: Dict) -> str:
    """Use Gemini to answer when no relevant documents found in knowledge base"""
    
    try:
        from llama_index.llms.gemini import Gemini
        from config.settings import config
        
        llm = Gemini(
            model=config.llm.main_model,
            api_key=config.llm.api_key
        )
        
        prompt = f"""You are a helpful AI assistant. The user asked: "{query}"

This question could not be answered from the knowledge base documents. Please provide a helpful, accurate answer based on your general knowledge.

Be concise and direct. If you don't know the answer, say so clearly."""

        response = await llm.acomplete(prompt)
        
        fallback_answer = f"""**AI Response** (No relevant documents found in knowledge base)

{response.text}

---
*Note: This answer is generated by AI based on general knowledge, not from your document collection. Consider adding relevant documents to your knowledge base for more specific information.*"""
        
        logger.info(f"Generated AI fallback answer ({len(response.text)} chars)")
        return fallback_answer
        
    except Exception as e:
        logger.error(f"AI fallback answer generation failed: {e}")
        return f"""No relevant documents found for your query: "{query}"

The AI fallback also encountered an error. Please try:
- Rephrasing your query
- Using more specific terms
- Checking if relevant documents exist in the knowledge base"""


async def batch_rerank_results(query: str, documents: List, top_k: int = 5) -> tuple:
    """Batch re-rank documents using SINGLE LLM call for all documents"""
    
    if not documents:
        return [], 0
    
    try:
        from llama_index.llms.gemini import Gemini
        from config.settings import config
        
        llm = Gemini(
            model=config.llm.main_model,
            api_key=config.llm.api_key,
            temperature=0.0
        )
        
        # Prepare batch prompt with ALL documents
        docs_text = ""
        for idx, doc in enumerate(documents[:top_k], 1):
            docs_text += f"\n--- Document {idx} ---\n"
            docs_text += f"Filename: {doc.filename}\n"
            docs_text += f"Content: {doc.content[:400]}...\n"
        
        # Single LLM call for all documents
        prompt = f"""You are a relevance evaluator. Evaluate if EACH document is relevant to the user's query.

User Query: "{query}"

{docs_text}

For EACH document, respond with ONLY ONE WORD: "RELEVANT" or "NOT_RELEVANT"

Format your response EXACTLY as:
Document 1: RELEVANT
Document 2: NOT_RELEVANT
Document 3: RELEVANT
...

Your evaluation:"""

        logger.info(f"ðŸ¤– Batch re-ranking {len(documents[:top_k])} documents with single LLM call...")
        
        response = await llm.acomplete(prompt)
        
        # Calculate tokens used (rough estimate)
        prompt_tokens = len(prompt.split()) * 1.3  # Rough token estimate
        response_tokens = len(response.text.split()) * 1.3
        total_tokens = int(prompt_tokens + response_tokens)
        
        # Parse batch response
        decisions = parse_batch_decisions(response.text, len(documents[:top_k]))
        
        # Filter relevant documents
        relevant_docs = []
        for idx, doc in enumerate(documents[:top_k]):
            if idx < len(decisions) and decisions[idx] == "RELEVANT":
                doc.metadata['rerank_verified'] = True
                doc.metadata['rerank_decision'] = 'llm_verified'
                relevant_docs.append(doc)
            else:
                logger.debug(f"   Filtered out: {doc.filename} - NOT_RELEVANT")
        
        logger.info(f"âœ… Batch re-ranking complete: {len(relevant_docs)}/{len(documents[:top_k])} relevant")
        
        return relevant_docs, total_tokens
        
    except Exception as e:
        logger.error(f"Batch re-ranking failed: {e}")
        return documents[:top_k], 0  # Fallback: return original documents


def parse_batch_decisions(response_text: str, expected_count: int) -> List[str]:
    """Parse LLM batch response into decisions"""
    decisions = []
    lines = response_text.strip().split('\n')
    
    for line in lines:
        line = line.strip().upper()
        if 'RELEVANT' in line:
            if 'NOT_RELEVANT' in line or 'NOT RELEVANT' in line:
                decisions.append("NOT_RELEVANT")
            else:
                decisions.append("RELEVANT")
    
    # If parsing failed or mismatch, assume all relevant
    if len(decisions) != expected_count:
        logger.warning(f"Batch parsing mismatch: got {len(decisions)}, expected {expected_count}")
        return ["RELEVANT"] * expected_count
    
    return decisions


async def smart_rerank_results(query: str, documents: List) -> tuple:
    """Smart re-ranking: LLM only when needed"""
    
    if not documents:
        return [], "no_documents", 0
    
    logger.info(f"ðŸ§  Smart re-ranking: analyzing {len(documents)} documents...")
    
    # Condition 1: High-quality database matches (>= 0.85 score)
    high_quality_db = [d for d in documents 
                       if "database" in d.source_method 
                       and d.similarity_score >= 0.85]
    
    if len(high_quality_db) >= 3:
        logger.info(f"âœ… Smart: Skipping LLM - {len(high_quality_db)} high-quality DB matches")
        for doc in high_quality_db[:5]:
            doc.metadata['rerank_decision'] = 'skipped_high_quality_db'
        return high_quality_db[:5], "skipped_high_quality_db", 0
    
    # Condition 2: Few candidates (no need for filtering) - BUT check quality first
    if len(documents) <= 2:
        # Only skip LLM if documents have reasonable quality scores
        all_decent_quality = all(d.similarity_score >= 0.70 for d in documents)

        if all_decent_quality:
            logger.info(f"âœ… Smart: Skipping LLM - only {len(documents)} candidates with decent quality (>= 0.70)")
            for doc in documents:
                doc.metadata['rerank_decision'] = 'skipped_few_candidates_high_quality'
            return documents, "skipped_few_candidates_high_quality", 0
        else:
            # Low quality scores - use LLM to verify relevance
            logger.info(f"ðŸ¤– Smart: Using LLM despite few candidates - low quality scores detected")
            relevant_docs, tokens = await batch_rerank_results(query, documents, top_k=5)
            return relevant_docs, "llm_verified_low_quality", tokens
    
    # Condition 3: All top documents have very high scores
    top_3 = documents[:3]
    all_high_score = all(d.similarity_score >= 0.80 for d in top_3)
    
    if all_high_score and len(documents) <= 3:
        logger.info(f"âœ… Smart: Skipping LLM - all {len(documents)} docs have high scores (>= 0.80)")
        for doc in documents:
            doc.metadata['rerank_decision'] = 'skipped_high_scores'
        return documents, "skipped_high_scores", 0
    
    # Otherwise: use batch LLM re-ranking
    logger.info(f"ðŸ¤– Smart: Using batch LLM re-ranking (conditions not met for skip)")
    relevant_docs, tokens = await batch_rerank_results(query, documents, top_k=5)
    return relevant_docs, "llm_verified", tokens


async def full_rerank_results(query: str, documents: List) -> tuple:
    """Full re-ranking: ALWAYS use LLM for all documents"""
    
    if not documents:
        return [], "no_documents", 0
    
    logger.info(f"ðŸš€ Full re-ranking: Using LLM for ALL {len(documents)} documents")
    
    # Always use batch LLM re-ranking (up to 10 documents)
    relevant_docs, tokens = await batch_rerank_results(query, documents, top_k=min(len(documents), 10))
    
    for doc in relevant_docs:
        doc.metadata['rerank_decision'] = 'full_llm_verified'
    
    return relevant_docs, "full_llm_verified", tokens


async def execute_search(system_components: Dict, query: str, rerank_mode: str = "smart"):
    """Execute the search pipeline with configurable re-ranking mode"""
    
    pipeline_start = time.time()
    
    # Initialize all time metrics
    extraction_time = 0
    rewrite_time = 0
    retrieval_time = 0
    fusion_time = 0
    rerank_time = 0
    answer_time = 0
    rerank_decision = ""
    tokens_used = 0
    
    try:
        # STAGE 0: Check if this is a direct question about the system itself
        query_lower = query.strip().lower()
        system_questions = ['who are you', 'what are you', 'what can you do', 
                           'who am i talking to', 'introduce yourself',
                           'tell me about yourself', 'what is your purpose']
        
        if any(q in query_lower for q in system_questions):
            logger.info("=" * 80)
            logger.info(f"SYSTEM QUESTION DETECTED: {query}")
            logger.info("Providing system information directly (skipping document search)")
            logger.info("=" * 80)
            
            from api.modules.search.models.schemas import QueryRewriteResult, PipelineEfficiency
            
            class SimpleEntityResult:
                def __init__(self):
                    self.entity = query
                    self.confidence = 1.0
                    self.method = "system_question"
                    self.alternatives = []
                    self.metadata = {"type": "system_question"}
            
            total_time = time.time() - pipeline_start
            
            return {
                "entity_result": SimpleEntityResult(),
                "rewrite_result": QueryRewriteResult(
                    original_query=query,
                    rewrites=[query],
                    method="none",
                    confidence=1.0,
                    metadata={"type": "system_question"}
                ),
                "fusion_result": type('FusionResult', (object,), {
                    'fused_results': [],
                    'final_count': 0,
                    'fusion_method': 'none',
                    'fusion_metadata': {}
                })(),
                "answer": await generate_system_info_answer(),
                "performance_metrics": {
                    "total_time": total_time,
                    "extraction_time": 0,
                    "rewrite_time": 0,
                    "retrieval_time": 0,
                    "fusion_time": 0,
                    "rerank_time": 0,
                    "answer_time": 0,
                    "pipeline_efficiency": PipelineEfficiency(),
                    "rerank_mode": rerank_mode,
                    "rerank_decision": "system_question",
                    "tokens_used": 0
                }
            }
        
        logger.info("=" * 80)
        logger.info(f"SEARCH PIPELINE STARTED: {query}")
        logger.info(f"Re-Ranking Mode: {rerank_mode.upper()}")
        logger.info("=" * 80)
        
        # STAGE 1: Entity Extraction
        logger.info("STAGE 1: Entity Extraction")
        extraction_start = time.time()
        entity_result = await system_components["entity_extractor"].extract_entity(query)
        extraction_time = time.time() - extraction_start
        logger.info(f"âœ“ Entity: '{entity_result.entity}' | Method: {entity_result.method} | Confidence: {entity_result.confidence:.2%} | Time: {extraction_time:.3f}s")
        
        # STAGE 2: Query Rewriting  
        logger.info("STAGE 2: Query Rewriting")
        rewrite_start = time.time()
        rewrite_result = await system_components["query_rewriter"].rewrite_query(
            query, entity_result.entity
        )
        rewrite_time = time.time() - rewrite_start
        logger.info(f"âœ“ Generated {len(rewrite_result.rewrites)} variants | Method: {rewrite_result.method} | Time: {rewrite_time:.3f}s")
        logger.info(f"  Variants: {rewrite_result.rewrites}")
        
        # STAGE 3: Hybrid Multi-Strategy Retrieval
        logger.info("STAGE 3: Hybrid Multi-Strategy Retrieval")
        retrieval_start = time.time()
        
        # Get required terms for content filtering
        required_terms = []
        if entity_result.entity != query.strip():
            entity_words = [word.lower() for word in entity_result.entity.split() 
                           if len(word) > 2 and word.lower() not in ['the', 'and', 'or']]
            required_terms = entity_words
        
        multi_retrieval_result = await system_components["retriever"].multi_retrieve(
            queries=rewrite_result.rewrites,
            extracted_entity=entity_result.entity,
            required_terms=required_terms
        )
        retrieval_time = time.time() - retrieval_start
        logger.info(f"âœ“ Retrieved {len(multi_retrieval_result.results)} candidates | Methods: {', '.join(multi_retrieval_result.methods_used)} | Time: {retrieval_time:.3f}s")
        
        # STAGE 4: Hybrid Results Fusion
        logger.info("STAGE 4: Hybrid Results Fusion")
        fusion_start = time.time()
        fusion_result = system_components["fusion_engine"].fuse_results(
            all_results=multi_retrieval_result.results,
            original_query=query,
            extracted_entity=entity_result.entity,
            required_terms=required_terms
        )
        fusion_time = time.time() - fusion_start
        logger.info(f"âœ“ Fused to {fusion_result.final_count} documents | Method: {fusion_result.fusion_method} | Time: {fusion_time:.3f}s")
        
        if fusion_result.fused_results:
            logger.info(f"  Top scores: {[f'{doc.similarity_score:.3f}' for doc in fusion_result.fused_results[:3]]}")
        
        # STAGE 5: Smart or Full Re-Ranking
        logger.info(f"STAGE 5: AI Re-Ranking ({rerank_mode.upper()} mode)")
        rerank_start = time.time()
        
        candidate_docs = fusion_result.fused_results[:10]
        
        if candidate_docs:
            if rerank_mode == RerankMode.SMART:
                relevant_docs, rerank_decision, tokens_used = await smart_rerank_results(query, candidate_docs)
            elif rerank_mode == RerankMode.FULL:
                relevant_docs, rerank_decision, tokens_used = await full_rerank_results(query, candidate_docs)
            else:
                # Default to smart
                relevant_docs, rerank_decision, tokens_used = await smart_rerank_results(query, candidate_docs)
        else:
            logger.info("  No candidates to re-rank")
            relevant_docs = []
            rerank_decision = "no_candidates"
            tokens_used = 0
        
        rerank_time = time.time() - rerank_start
        logger.info(f"âœ“ Re-ranking complete | Mode: {rerank_mode} | Decision: {rerank_decision} | Found {len(relevant_docs)} relevant | Tokens: {tokens_used} | Time: {rerank_time:.3f}s")
        
        # STAGE 6: Quality Filtering
        logger.info("STAGE 6: Quality Filtering")
        quality_results = [r for r in relevant_docs if r.similarity_score >= MIN_RELEVANCE_THRESHOLD]
        
        if quality_results:
            logger.info(f"âœ“ {len(quality_results)} documents passed quality threshold ({MIN_RELEVANCE_THRESHOLD})")
        else:
            best_score = max([r.similarity_score for r in fusion_result.fused_results]) if fusion_result.fused_results else 0
            logger.info(f"âœ— No documents passed quality threshold (best: {best_score:.3f}, threshold: {MIN_RELEVANCE_THRESHOLD})")
        
        # STAGE 7: Final Answer Generation
        logger.info("STAGE 7: Answer Generation")
        answer_start = time.time()
        
        if not quality_results:
            logger.info("  Using AI fallback (no quality documents)")
            answer = await generate_ai_fallback_answer(query, system_components)
            fusion_result.fused_results = []
            fusion_result.final_count = 0
        else:
            logger.info(f"  Generating answer from {len(quality_results)} documents")
            answer = generate_answer(query, quality_results, entity_result, rerank_mode, rerank_decision)
            fusion_result.fused_results = quality_results
            fusion_result.final_count = len(quality_results)
        
        answer_time = time.time() - answer_start
        logger.info(f"âœ“ Answer generated | Length: {len(answer)} chars | Time: {answer_time:.3f}s")
        
        total_time = time.time() - pipeline_start
        
        # Calculate pipeline efficiency
        from api.modules.search.models.schemas import PipelineEfficiency
        pipeline_efficiency = PipelineEfficiency(
            extraction_pct=(extraction_time / total_time) * 100,
            rewrite_pct=(rewrite_time / total_time) * 100,
            retrieval_pct=(retrieval_time / total_time) * 100,
            fusion_pct=(fusion_time / total_time) * 100,
            rerank_pct=(rerank_time / total_time) * 100,
            answer_pct=(answer_time / total_time) * 100
        )
        
        logger.info("=" * 80)
        logger.info(f"SEARCH PIPELINE COMPLETED")
        logger.info(f"Total Time: {total_time:.3f}s | Results: {fusion_result.final_count}")
        logger.info(f"Re-Ranking: {rerank_mode} mode | Decision: {rerank_decision} | Tokens: {tokens_used}")
        logger.info(f"Breakdown: Extract={extraction_time:.3f}s | Rewrite={rewrite_time:.3f}s | Retrieve={retrieval_time:.3f}s | Fuse={fusion_time:.3f}s | Rerank={rerank_time:.3f}s | Answer={answer_time:.3f}s")
        logger.info("=" * 80)
        
        return {
            "entity_result": entity_result,
            "rewrite_result": rewrite_result,
            "fusion_result": fusion_result,
            "answer": answer,
            "performance_metrics": {
                "total_time": total_time,
                "extraction_time": extraction_time,
                "rewrite_time": rewrite_time,
                "retrieval_time": retrieval_time,
                "fusion_time": fusion_time,
                "rerank_time": rerank_time,
                "answer_time": answer_time,
                "pipeline_efficiency": pipeline_efficiency,
                "rerank_mode": rerank_mode,
                "rerank_decision": rerank_decision,
                "tokens_used": tokens_used
            }
        }
        
    except Exception as e:
        logger.error(f"Search pipeline failed: {e}", exc_info=True)
        raise


def generate_answer(query: str, results, entity_result, rerank_mode: str = "smart", rerank_decision: str = "") -> str:
    """Generate answer from search results with re-ranking info"""
    
    if not results:
        return f"""No relevant information found for your query: "{query}"

Search Summary:
- Entity extracted: "{entity_result.entity}" (confidence: {entity_result.confidence:.1%})
- Method used: {entity_result.method}
- Search strategy: Hybrid (Vector + Database) + AI Re-Ranking ({rerank_mode} mode)

Suggestions:
- Try rephrasing your query
- Use more specific terms
- Check if the information exists in the knowledge base"""

    # Categorize results by quality
    database_results = [r for r in results if "database" in r.source_method]
    vector_results = [r for r in results if ("vector" in r.source_method or "llamaindex" in r.source_method)]
    
    high_quality = [r for r in results if r.similarity_score >= 0.8]
    medium_quality = [r for r in results if 0.7 <= r.similarity_score < 0.8]
    
    # Generate contextual answer
    answer_parts = []
    
    # Header with hybrid search success
    answer_parts.append(f"Found {len(results)} AI-verified relevant documents for '{entity_result.entity}':")
    
    # Show source distribution
    if database_results and vector_results:
        answer_parts.append(f"\nHybrid Search: {len(database_results)} exact matches + {len(vector_results)} semantic matches")
    elif database_results:
        answer_parts.append(f"\nDatabase Search: {len(database_results)} exact matches found")
    elif vector_results:
        answer_parts.append(f"\nVector Search: {len(vector_results)} semantic matches found")
    
    # High quality results summary
    if high_quality:
        answer_parts.append(f"\n\nPrimary Information ({len(high_quality)} high-confidence documents):")
        for i, result in enumerate(high_quality[:3], 1):
            preview = result.content[:200] + "..." if len(result.content) > 200 else result.content
            source_indicator = "Database" if "database" in result.source_method else "Vector"
            answer_parts.append(f"{i}. [{source_indicator}] {result.filename} (score: {result.similarity_score:.3f})")
            answer_parts.append(f"   {preview}")
    
    # Medium quality results summary  
    if medium_quality and len(high_quality) < 3:
        needed = 3 - len(high_quality)
        answer_parts.append(f"\n\nAdditional Information ({len(medium_quality)} medium-confidence documents):")
        for i, result in enumerate(medium_quality[:needed], len(high_quality) + 1):
            preview = result.content[:150] + "..." if len(result.content) > 150 else result.content
            source_indicator = "Database" if "database" in result.source_method else "Vector"
            answer_parts.append(f"{i}. [{source_indicator}] {result.filename} (score: {result.similarity_score:.3f})")
            answer_parts.append(f"   {preview}")
    
    # Search intelligence summary with re-ranking info
    answer_parts.append(f"\n\nSearch Intelligence:")
    answer_parts.append(f"- Entity analysis: {entity_result.method} extraction")
    answer_parts.append(f"- Search approach: Hybrid (Database + Vector) + AI Re-Ranking")
    answer_parts.append(f"- Re-Ranking mode: {rerank_mode}")
    
    # Add re-ranking decision info
    if rerank_decision == "skipped_high_quality_db":
        answer_parts.append(f"- Re-Ranking decision: Skipped (high-quality DB matches)")
    elif rerank_decision == "skipped_few_candidates":
        answer_parts.append(f"- Re-Ranking decision: Skipped (few candidates)")
    elif rerank_decision == "skipped_high_scores":
        answer_parts.append(f"- Re-Ranking decision: Skipped (all high scores)")
    elif rerank_decision == "llm_verified":
        answer_parts.append(f"- Re-Ranking decision: AI verified (needed for accuracy)")
    elif rerank_decision == "full_llm_verified":
        answer_parts.append(f"- Re-Ranking decision: Full AI verification")
    
    answer_parts.append(f"- Best match confidence: {max(r.similarity_score for r in results):.1%}")
    
    return "\n".join(answer_parts)


@router.post("", response_model=SearchResponse, responses={500: {"model": ErrorResponse}})
async def search(
    request: SearchRequest,
    components: SystemComponents = Depends(get_system_components)
):
    """
    Execute hybrid search with Smart or Full AI Re-Ranking.
    
    Pipeline:
    1. Detects if query is about the system itself
    2. Extracts entities and rewrites queries for better retrieval
    3. Searches knowledge base using hybrid approach (database + vector)
    4. Re-Ranking modes:
       - Smart (default): Uses AI only when needed, auto-skips for exact matches
       - Full: Always verifies all documents with AI for maximum accuracy
    5. Filters results by quality threshold
    6. Returns document-based answer or AI-generated response
    
    - **query**: Search query text (1-1000 characters)
    - **max_results**: Maximum results to return (1-100, default: 20)
    - **similarity_threshold**: Optional similarity threshold (0.0-1.0)
    - **rerank_mode**: Re-ranking mode (smart or full, default: smart)
    """
    
    try:
        system_components = components.get_components()
        
        result = await execute_search(
            system_components, 
            request.query,
            rerank_mode=request.rerank_mode.value if request.rerank_mode else "smart"
        )
        
        # Convert to response model
        from api.modules.search.models.schemas import (
            EntityResult, QueryRewriteResult, DocumentResult, PerformanceMetrics
        )
        
        return SearchResponse(
            success=True,
            query=request.query,
            entity_result=EntityResult(
                entity=result["entity_result"].entity,
                confidence=result["entity_result"].confidence,
                method=result["entity_result"].method,
                alternatives=result["entity_result"].alternatives,
                metadata=result["entity_result"].metadata
            ),
            rewrite_result=QueryRewriteResult(
                original_query=result["rewrite_result"].original_query,
                rewrites=result["rewrite_result"].rewrites,
                method=result["rewrite_result"].method,
                confidence=result["rewrite_result"].confidence,
                metadata=result["rewrite_result"].metadata
            ),
            results=[
                DocumentResult(
                    filename=doc.filename,
                    content=doc.content,
                    full_content=doc.full_content,
                    similarity_score=doc.similarity_score,
                    source_method=doc.source_method,
                    document_id=doc.document_id,
                    chunk_index=doc.chunk_index,
                    metadata=doc.metadata
                )
                for doc in result["fusion_result"].fused_results[:request.max_results]
            ],
            answer=result["answer"],
            total_results=len(result["fusion_result"].fused_results),
            performance_metrics=PerformanceMetrics(**result["performance_metrics"])
        )
        
    except Exception as e:
        logger.error(f"Search endpoint error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))